# ğŸ¤– AI PDF Chatbot - Powered by Gemini 2.5 Pro

A modern, professional PDF Q&A chatbot built with FastAPI, LangChain, and Google's Gemini 2.5 Pro AI. Upload any PDF document and ask questions in natural language!

![Python](https://img.shields.io/badge/Python-3.12-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-green)
![LangChain](https://img.shields.io/badge/LangChain-1.0+-orange)
![Gemini](https://img.shields.io/badge/Gemini-2.5%20Pro-purple)

## âœ¨ Features

- ğŸ“„ **PDF Upload & Processing** - Drag & drop or click to upload
- ğŸ§  **AI-Powered Q&A** - Powered by Gemini 2.5 Pro (1M token context)
- âš¡ **Fast Performance** - Local embeddings with HuggingFace
- ğŸ’¬ **Natural Language** - Ask questions in plain English
- ğŸ¨ **Modern UI** - Beautiful, responsive interface
- ğŸ”’ **Secure** - Local processing with API-based AI

## ğŸš€ Quick Start

### Prerequisites

- Python 3.12+
- Google Gemini API Key

### Installation

1. **Clone the repository**
```bash
git clone <your-repo-url>
cd langchain_fastapi_pdfbot
```

2. **Create virtual environment**
```bash
python -m venv venv
venv\Scripts\activate  # Windows
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Set up environment variables**

Create a `.env` file:
```env
LLM_PROVIDER=gemini
GOOGLE_API_KEY=your_google_api_key_here
```

5. **Run the application**
```bash
uvicorn main:app --reload
```

6. **Open your browser**
```
http://127.0.0.1:8000
```

## ğŸ“ Project Structure

```
langchain_fastapi_pdfbot/
â”œâ”€â”€ main.py                 # FastAPI application
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ .env                   # Environment variables
â”œâ”€â”€ static/                # Frontend files
â”‚   â”œâ”€â”€ index.html        # Main HTML page
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ style.css     # Styles
â”‚   â””â”€â”€ js/
â”‚       â””â”€â”€ app.js        # JavaScript logic
â”œâ”€â”€ chains/               # LangChain chains
â”‚   â”œâ”€â”€ pdf_qa_chain.py  # Q&A chain
â”‚   â””â”€â”€ prompt_templates.py
â”œâ”€â”€ utils/               # Utility functions
â”‚   â”œâ”€â”€ pdf_loader.py   # PDF processing
â”‚   â”œâ”€â”€ embeddings.py   # Embedding models
â”‚   â””â”€â”€ vector_store.py # Vector database
â”œâ”€â”€ uploads/            # Uploaded PDFs
â””â”€â”€ chroma_db/         # Vector database storage
```

## ğŸ¯ How It Works

1. **Upload PDF** - User uploads a PDF document
2. **Process** - PDF is split into chunks and embedded using HuggingFace
3. **Store** - Embeddings are stored in ChromaDB vector database
4. **Ask** - User asks questions in natural language
5. **Retrieve** - Relevant chunks are retrieved from vector store
6. **Answer** - Gemini 2.5 Pro generates comprehensive answers

## ğŸ› ï¸ Tech Stack

### Backend
- **FastAPI** - Modern Python web framework
- **LangChain** - LLM application framework
- **Google Gemini 2.5 Pro** - AI model for Q&A
- **HuggingFace** - Local embeddings (sentence-transformers)
- **ChromaDB** - Vector database

### Frontend
- **HTML5** - Structure
- **CSS3** - Modern styling with animations
- **Vanilla JavaScript** - No frameworks needed

## ğŸ“Š API Endpoints

### `POST /upload/`
Upload and process a PDF document
- **Input**: PDF file (multipart/form-data)
- **Output**: Processing status, chunk count

### `POST /ask/`
Ask a question about the uploaded PDF
- **Input**: `pdf_path`, `question` (form data)
- **Output**: AI-generated answer

### `GET /docs`
Interactive API documentation (Swagger UI)

## ğŸ¨ UI Features

- **Drag & Drop Upload** - Easy file upload
- **Real-time Chat** - Instant Q&A interface
- **Markdown Support** - Formatted responses
- **Loading States** - Visual feedback
- **Responsive Design** - Works on all devices
- **Smooth Animations** - Professional feel

## ğŸ”§ Configuration

### Environment Variables

```env
# LLM Provider (gemini or openai)
LLM_PROVIDER=gemini

# Google Gemini API Key
GOOGLE_API_KEY=your_api_key_here

# Optional: OpenAI API Key (if using OpenAI)
OPENAI_API_KEY=your_openai_key_here
```

### Available Models

- `gemini-2.5-pro` - Best quality (default)
- `gemini-2.5-flash` - Faster responses
- `gemini-2.0-flash` - Older version

## ğŸ“ˆ Performance

- **Upload Time**: 20-30 seconds (one-time processing)
- **Query Time**: 2-5 seconds (fast retrieval + AI)
- **Context Window**: 1M tokens (Gemini 2.5 Pro)
- **Output Limit**: 65K tokens

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“ License

This project is licensed under the MIT License.

## ğŸ‘¨â€ğŸ’» Author

Built with â¤ï¸ by [Your Name]

## ğŸ™ Acknowledgments

- Google Gemini AI
- LangChain Team
- FastAPI Community
- HuggingFace

---

**Note**: Make sure to keep your API keys secure and never commit them to version control!
